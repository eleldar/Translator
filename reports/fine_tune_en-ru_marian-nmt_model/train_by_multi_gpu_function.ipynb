{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95968f92",
   "metadata": {},
   "source": [
    "# Fine tuning Marian-NMT en-ru model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea507ed9",
   "metadata": {},
   "source": [
    "## Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a791c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers[sentencepiece]\n",
    "!pip install sacrebleu\n",
    "!pip install accelerate\n",
    "!pip install openpyxl\n",
    "!apt install git-lfs\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим репозиторий; нужен для предобработки\n",
    "!git clone https://github.com/eleldar/Translator.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c64e88eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'opus-mt-en-ru'...\n",
      "remote: Enumerating objects: 58, done.\u001b[K\n",
      "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 58 (delta 30), reused 58 (delta 30), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (58/58), done.\n",
      "Filtering content: 100% (2/2), 829.76 MiB | 20.66 MiB/s, done.\n",
      "\u001b[H\u001b[2JREADME.md    pytorch_model.bin\tsource.spm  tokenizer_config.json\n",
      "config.json  rust_model.ot\ttarget.spm  vocab.json\n"
     ]
    }
   ],
   "source": [
    "# загрузка исходной модели\n",
    "!git clone https://huggingface.co/Helsinki-NLP/opus-mt-en-ru && ls opus-mt-en-ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db674b65",
   "metadata": {},
   "source": [
    "## Настройка git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208e5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"eleldar@mail.ru\"\n",
    "!git config --global user.name \"eleldar\"\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde800c5",
   "metadata": {},
   "source": [
    "## Импортирование зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d1bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from time import gmtime, strftime\n",
    "from huggingface_hub import Repository\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "import datasets\n",
    "from datasets import (\n",
    "    Dataset, DatasetDict, load_dataset, load_metric,\n",
    "    concatenate_datasets, interleave_datasets\n",
    ")\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW, AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,\n",
    "    get_constant_schedule, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup, \n",
    "    get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, \n",
    "    get_polynomial_decay_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a4e60",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510feed",
   "metadata": {},
   "source": [
    "### Общий корпус длинных предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0853fc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On Monday, scientists from the Stanford Univer...</td>\n",
       "      <td>В понедельник ученые из Медицинской школы Стэн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead researchers say this may bring early dete...</td>\n",
       "      <td>Ведущие исследователи утверждают, что он может...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The JAS 39C Gripen crashed onto a runway at ar...</td>\n",
       "      <td>Приблизительно в 9:30 по местному времени (02:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pilot was identified as Squadron Leader Di...</td>\n",
       "      <td>Личность пилота была установлена. Им оказался ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Local media reports an airport fire vehicle ro...</td>\n",
       "      <td>Местные СМИ сообщают, что в аэропорту по пути ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>As the areas are sparsely populated, and light...</td>\n",
       "      <td>Так как эти районы являются малонаселенными, и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>Japanese work culture is more hierarchical and...</td>\n",
       "      <td>Японская культура труда иерархичнее и формальн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>Suits are standard business attire, and cowork...</td>\n",
       "      <td>Костюмы являются стандартной деловой одеждой, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>Workplace harmony is crucial, emphasizing grou...</td>\n",
       "      <td>Чрезвычайно важна гармония на рабочем месте, а...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>Workers must often get their superiors' approv...</td>\n",
       "      <td>Обычно работники должны получить разрешение на...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2009 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     en  \\\n",
       "0     On Monday, scientists from the Stanford Univer...   \n",
       "1     Lead researchers say this may bring early dete...   \n",
       "2     The JAS 39C Gripen crashed onto a runway at ar...   \n",
       "3     The pilot was identified as Squadron Leader Di...   \n",
       "4     Local media reports an airport fire vehicle ro...   \n",
       "...                                                 ...   \n",
       "2004  As the areas are sparsely populated, and light...   \n",
       "2005  Japanese work culture is more hierarchical and...   \n",
       "2006  Suits are standard business attire, and cowork...   \n",
       "2007  Workplace harmony is crucial, emphasizing grou...   \n",
       "2008  Workers must often get their superiors' approv...   \n",
       "\n",
       "                                                     ru  \n",
       "0     В понедельник ученые из Медицинской школы Стэн...  \n",
       "1     Ведущие исследователи утверждают, что он может...  \n",
       "2     Приблизительно в 9:30 по местному времени (02:...  \n",
       "3     Личность пилота была установлена. Им оказался ...  \n",
       "4     Местные СМИ сообщают, что в аэропорту по пути ...  \n",
       "...                                                 ...  \n",
       "2004  Так как эти районы являются малонаселенными, и...  \n",
       "2005  Японская культура труда иерархичнее и формальн...  \n",
       "2006  Костюмы являются стандартной деловой одеждой, ...  \n",
       "2007  Чрезвычайно важна гармония на рабочем месте, а...  \n",
       "2008  Обычно работники должны получить разрешение на...  \n",
       "\n",
       "[2009 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_url = 'https://github.com/eleldar/Translator/blob/master/test_dataset/flores101_dataset/101_languages.xlsx?raw=true'\n",
    "normal_df = pd.read_excel(normal_url)[[\"eng\", \"rus\"]].rename(columns={\"eng\": \"en\", \"rus\": \"ru\"})\n",
    "normal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb669d",
   "metadata": {},
   "source": [
    "### Общий корпус кратких предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613c28bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning!</td>\n",
       "      <td>Доброе утро!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did the alarm clock go off?</td>\n",
       "      <td>Будильник звонил?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's time to get up.</td>\n",
       "      <td>Время вставать!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Get up soon.</td>\n",
       "      <td>Вставай быстрее.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are you awake?</td>\n",
       "      <td>Ты проснулся?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>Never say die.</td>\n",
       "      <td>Никогда не сдавайся.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>Let bygones be bygones.</td>\n",
       "      <td>Пусть прошедшее будет прошедшим.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>Better than nothing.</td>\n",
       "      <td>Лучше, чем ничего.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>A fair-weather friend.</td>\n",
       "      <td>Ненадёжный друг.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>Every little bit helps.</td>\n",
       "      <td>Каждый маленький кусочек поможет.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2664 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               en                                 ru\n",
       "0                   Good morning!                       Доброе утро!\n",
       "1     Did the alarm clock go off?                  Будильник звонил?\n",
       "2            It's time to get up.                    Время вставать!\n",
       "3                    Get up soon.                   Вставай быстрее.\n",
       "4                  Are you awake?                      Ты проснулся?\n",
       "...                           ...                                ...\n",
       "2659               Never say die.               Никогда не сдавайся.\n",
       "2660      Let bygones be bygones.   Пусть прошедшее будет прошедшим.\n",
       "2661         Better than nothing.                 Лучше, чем ничего.\n",
       "2662       A fair-weather friend.                   Ненадёжный друг.\n",
       "2663      Every little bit helps.  Каждый маленький кусочек поможет.\n",
       "\n",
       "[2664 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_url = 'https://github.com/eleldar/Translator/blob/master/test_dataset/normal.xlsx?raw=true'\n",
    "short_df = pd.read_excel(short_url).rename(columns={\"en_sent\": \"en\", \"ru_sent\": \"ru\"})\n",
    "short_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad0d97",
   "metadata": {},
   "source": [
    "### Предметный корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee21a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please contact us as soon as possible</td>\n",
       "      <td>пожалуйста, свяжитесь с нами как можно скорее.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you would like to have an additional inform...</td>\n",
       "      <td>если вы хотите получить дополнительную информа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This task is difficult for me.</td>\n",
       "      <td>Это задание трудное для меня.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our task is to increase channel activity in th...</td>\n",
       "      <td>Наша задача состоит в том, чтобы увеличить акт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My employee never completes tasks on time.</td>\n",
       "      <td>Мой сотрудник никогда не делает задания в срок.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>Disable anti-virus firewall and open up port 443.</td>\n",
       "      <td>Отключите брандмауэр для защиты от вирусов и о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>We need to find a way to break through that si...</td>\n",
       "      <td>Нам нужно найти способ обойти брандмауэр сайта.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>Several representatives supported incremental ...</td>\n",
       "      <td>Ряд представителей высказались за поэтапное ра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>Incremental development increases affordabilit...</td>\n",
       "      <td>Поэтапное развитие повышает доступность за сче...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>Incremental development: This term refers to t...</td>\n",
       "      <td>Поэтапное развитие: этот термин означает посте...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1038 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     en  \\\n",
       "0                 Please contact us as soon as possible   \n",
       "1     If you would like to have an additional inform...   \n",
       "2                        This task is difficult for me.   \n",
       "3     Our task is to increase channel activity in th...   \n",
       "4            My employee never completes tasks on time.   \n",
       "...                                                 ...   \n",
       "1033  Disable anti-virus firewall and open up port 443.   \n",
       "1034  We need to find a way to break through that si...   \n",
       "1035  Several representatives supported incremental ...   \n",
       "1036  Incremental development increases affordabilit...   \n",
       "1037  Incremental development: This term refers to t...   \n",
       "\n",
       "                                                     ru  \n",
       "0        пожалуйста, свяжитесь с нами как можно скорее.  \n",
       "1     если вы хотите получить дополнительную информа...  \n",
       "2                         Это задание трудное для меня.  \n",
       "3     Наша задача состоит в том, чтобы увеличить акт...  \n",
       "4       Мой сотрудник никогда не делает задания в срок.  \n",
       "...                                                 ...  \n",
       "1033  Отключите брандмауэр для защиты от вирусов и о...  \n",
       "1034    Нам нужно найти способ обойти брандмауэр сайта.  \n",
       "1035  Ряд представителей высказались за поэтапное ра...  \n",
       "1036  Поэтапное развитие повышает доступность за сче...  \n",
       "1037  Поэтапное развитие: этот термин означает посте...  \n",
       "\n",
       "[1038 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_url = 'https://github.com/eleldar/Translator/blob/master/test_dataset/corrected_vocab.xlsx?raw=true'\n",
    "subject_df = pd.read_excel(subject_url).drop(columns=['en_keys', 'ru_keys']).rename(columns={\"en_sent\": \"en\", \"ru_sent\": \"ru\"})\n",
    "subject_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f0b1e",
   "metadata": {},
   "source": [
    "### Тестовый корпус (из модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054b1f40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Have you ever been to Switzerland?</td>\n",
       "      <td>Ты уже бывал в Швейцарии?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She learned quickly.</td>\n",
       "      <td>Она быстро училась.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No one tried to stop me.</td>\n",
       "      <td>Никто не пытался меня остановить.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guilds were an important part of society in th...</td>\n",
       "      <td>Гильдии были важной частью общества в Средние ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You say that it is your custom to burn widows....</td>\n",
       "      <td>Вы говорите, что сжигать вдов - ваш обычай. Оч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>You may sit anywhere you like.</td>\n",
       "      <td>Ты можешь сесть где хочешь.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>There's a lot I want to talk to you about.</td>\n",
       "      <td>Я о многом хочу с вами поговорить.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Let me tell Tom.</td>\n",
       "      <td>Позволь мне рассказать Тому.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>At twilight, snow looks blue.</td>\n",
       "      <td>В сумерках снег кажется синим.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>How much money does he have?</td>\n",
       "      <td>Сколько у него денег?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     en  \\\n",
       "0                    Have you ever been to Switzerland?   \n",
       "1                                  She learned quickly.   \n",
       "2                              No one tried to stop me.   \n",
       "3     Guilds were an important part of society in th...   \n",
       "4     You say that it is your custom to burn widows....   \n",
       "...                                                 ...   \n",
       "4995                     You may sit anywhere you like.   \n",
       "4996         There's a lot I want to talk to you about.   \n",
       "4997                                   Let me tell Tom.   \n",
       "4998                      At twilight, snow looks blue.   \n",
       "4999                       How much money does he have?   \n",
       "\n",
       "                                                     ru  \n",
       "0                             Ты уже бывал в Швейцарии?  \n",
       "1                                   Она быстро училась.  \n",
       "2                     Никто не пытался меня остановить.  \n",
       "3     Гильдии были важной частью общества в Средние ...  \n",
       "4     Вы говорите, что сжигать вдов - ваш обычай. Оч...  \n",
       "...                                                 ...  \n",
       "4995                        Ты можешь сесть где хочешь.  \n",
       "4996                 Я о многом хочу с вами поговорить.  \n",
       "4997                       Позволь мне рассказать Тому.  \n",
       "4998                     В сумерках снег кажется синим.  \n",
       "4999                              Сколько у него денег?  \n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url = 'https://github.com/eleldar/Translator/blob/master/test_dataset/test_opus_en-ru_dataset.xlsx?raw=true'\n",
    "test_df = pd.read_excel(test_url).drop(columns=['Unnamed: 0'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19978223",
   "metadata": {},
   "source": [
    "## Предобработка данных\n",
    "\n",
    "> Требуется замена символов юникода, т.к. встроенный токенизатор этого не выполняет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2243bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/home'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a969e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# переключим на каталог с импортируемыми модулями\n",
    "os.chdir('/mnt/home/Translator/OpenAPI/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be9b981a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/home/Translator/OpenAPI'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dbd0ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md    api\t\t   main.py  requirements.txt\r\n",
      "__pycache__  documented_endpoints  models   venv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5327874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортировали\n",
    "from api.tools.preprocess import get_commands, preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "061902fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('～', '\\\\~'), ('〉', '\\\\>'), ('？', '\\\\?'), ('；', ';'), ('。 *', '. ')],\n",
       " [('～', '\\\\~'), ('〉', '\\\\>'), ('？', '\\\\?'), ('；', ';'), ('。 *', '. ')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# словарь команд для предобработки на основе файла с расширением направления перевода и checkpoints\n",
    "checkpoints = {'en-ru', 'ar-ru', 'ru-ar', 'ru-en', 'en-ar', 'ar-en'}\n",
    "commands = get_commands(checkpoints)\n",
    "list(commands['en-ru'])[:5], list(commands['ru-en'])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ef20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# замена спецсимволов\n",
    "# normalisation = lambda text: 1 # preprocess_text(commands['en-ru'], text['en_sent']) if direct in commands else text['en_sent']\n",
    "def normalisation(text):\n",
    "    text['en'] = preprocess_text(commands['en-ru'], text['en'])\n",
    "    text['ru'] = preprocess_text(commands['ru-en'], text['ru'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cd025eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вернули рабочую директорию\n",
    "os.chdir('/mnt/home')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a95fa2",
   "metadata": {},
   "source": [
    "## Сборка наборов данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fa13a",
   "metadata": {},
   "source": [
    "### Создадим объекты Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95c279f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f13cefb8f12455aa370f9746526c46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2009 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ru'],\n",
       "    num_rows: 2009\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Общий корпус длинных предложений\n",
    "# normal_df\n",
    "normal_dataset = Dataset.from_pandas(normal_df)\n",
    "normal_dataset = normal_dataset.map(normalisation)\n",
    "normal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262d8c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e98581d13af4eafa776b9f59ee9d8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2664 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ru'],\n",
       "    num_rows: 2664\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Общий корпус кратких предложений\n",
    "# short_df\n",
    "short_dataset = Dataset.from_pandas(short_df)\n",
    "short_dataset = short_dataset.map(normalisation)\n",
    "short_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f0ed602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bc6f8e065e46bc89bb93b6c36f0522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1038 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ru'],\n",
       "    num_rows: 1038\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предметный корпус\n",
    "# subject_df\n",
    "subject_dataset = Dataset.from_pandas(subject_df).shuffle()\n",
    "subject_dataset = subject_dataset.map(normalisation)\n",
    "subject_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85acde13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e855e359472f4eb7a0f6d72f64f80eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ru'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тестовый корпус\n",
    "# test_df\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(normalisation)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259289da",
   "metadata": {},
   "source": [
    "### Объединим обучающую часть предметного и тестовые набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28875306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# целевой \"словарь\"\n",
    "split_datasets = DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d06a3750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    normal: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 2009\n",
       "    })\n",
       "    short: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 2664\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['normal'] = normal_dataset\n",
    "split_datasets['short'] = short_dataset\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4339028b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 830\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 208\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_and_test = subject_dataset.train_test_split(test_size=0.2)\n",
    "sub_train_and_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "784d9595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 4170\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 830\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = test_dataset.train_test_split(test_size=0.166)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88b32583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    normal: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 2009\n",
       "    })\n",
       "    short: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 2664\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 1660\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 208\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['train'] = interleave_datasets(\n",
    "    [sub_train_and_test['train'], tmp['test']]\n",
    ").shuffle()\n",
    "split_datasets['validation'] = sub_train_and_test.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8323527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Расскоментровать для использования всего модельного датасета для обучения; также в функции приедется изменить методы оценки\n",
    "# split_datasets['train'] = concatenate_datasets(\n",
    "#     [sub_train_and_test['train'], test_dataset]\n",
    "# ).shuffle()\n",
    "# split_datasets['validation'] = sub_train_and_test.pop(\"test\")\n",
    "# split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc7e8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    normal: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 2009\n",
       "    })\n",
       "    short: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 2664\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 1660\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 208\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'ru'],\n",
       "        num_rows: 4170\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['test'] = tmp['train']\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d7a03",
   "metadata": {},
   "source": [
    "## Функция обучения\n",
    "\n",
    "> Не поддерживает использование в качестве метода либо в цикл (проверено эмпирическим путем), т.к. используется параллельное использование нескольких GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedulers = ['get_constant_schedule', 'get_constant_schedule_with_warmup',\n",
    "                 'get_cosine_schedule_with_warmup', 'get_cosine_with_hard_restarts_schedule_with_warmup',\n",
    "                 'get_linear_schedule_with_warmup', 'get_polynomial_decay_schedule_with_warmup',\n",
    "                 'torch_optim_lr_scheduler_one_cycle_lr']\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \"num_epochs\": 2,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 32, \n",
    "    \"model_checkpoint\": \"opus-mt-en-ru\",\n",
    "    \"max_input_length\": 128,\n",
    "    \"max_target_length\": 128,\n",
    "    \"max_generate_length\": 128,    \n",
    "    \"output_dir\": f'experiences/fine_tuned_en_ru_model_{strftime(\"%Y-%m-%d_%H-%M-%S\", gmtime())}',\n",
    "    \"file_scores\": 'scores.txt',\n",
    "    \"scheduler\": lr_schedulers[0], # настраиваемый параметр\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparameters[\"model_checkpoint\"], return_tensors=\"pt\")\n",
    "\n",
    "def preprocess_function(examples, hyperparameters=hyperparameters, tokenizer=tokenizer):\n",
    "    '''Получение IDs'''\n",
    "    model_inputs = tokenizer(examples[\"en\"], max_length=hyperparameters['max_input_length'], truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"ru\"], max_length=hyperparameters['max_target_length'], truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"] # присвоили исходному языку IDs целевого языка\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def postprocess(predictions, labels, tokenizer=tokenizer):\n",
    "    '''Получение текста из IDs'''\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    # Декодированные токены из IDs, спрогнозированные моделью\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Замена -100 в метках, так как их нельзя декодировать.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Декодированные метки токены из IDs, являющиеся эталонным переводом\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Пост-обрабработка, т.к. для прогноза нужен список, а для эталона список списков\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "\n",
    "def evaluate(model, accelerator, examples, epoch='base', note=\"sub\", hyperparameters=hyperparameters):\n",
    "    '''Оценка'''\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    model.eval()\n",
    "    for batch in tqdm(examples):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=hyperparameters[\"max_generate_length\"],\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "        # Необходимое выравнивание для заполнения прогнозов и меток для метода accelerator.gather()\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        # подготовка данных для оценки\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        # примечение пакетной метрики\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    results = metric.compute()\n",
    "    response = f\"{note}_score for {epoch} epoch: {results['score']}\\n\"\n",
    "    with open(f\"{hyperparameters['output_dir']}_{hyperparameters['scheduler']}/{hyperparameters['file_scores']}\", 'a') as file:\n",
    "        file.write(response)\n",
    "    print(f\"{note}_score for epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "    \n",
    "\n",
    "def get_image(hyperparameters):\n",
    "    '''Создание и сохранение графика'''\n",
    "    with open(f\"{hyperparameters['output_dir']}_{hyperparameters['scheduler']}/{hyperparameters['file_scores']}\") as f:\n",
    "        score = f.readlines()\n",
    "    sub = [float(i.strip().split(': ')[1]) for i in score[0::4][0::4]]\n",
    "    normal = [float(i.strip().split(': ')[1]) for i in score[0::4][1::4]]\n",
    "    short = [float(i.strip().split(': ')[1]) for i in score[0::4][2::4]]\n",
    "    test = [float(i.strip().split(': ')[1]) for i in score[0::4][3::4]]\n",
    "    X = [i for i in range(hyperparameters[\"num_epochs\"] + 1)]\n",
    "    Y = [i for i in range(0, 61)]\n",
    "    score_df = pd.DataFrame({'Предметный': sub, 'Обычные': normal, 'Короткие': short, 'Модельные': test})\n",
    "    mx_sub = max(sub)\n",
    "    inx = sub.index(mx_sub)\n",
    "    modscore = test[inx]\n",
    "    img = score_df.plot(xticks=X, yticks=Y, style='^', figsize=(15,12));\n",
    "    img.axvline(inx, color='grey')\n",
    "    img.legend(loc='lower left')\n",
    "    img.set_xlabel(\"Epochs\")\n",
    "    img.set_ylabel(\"BLEU\")\n",
    "    img.annotate(f'sub {mx_sub:.2f}', xy=(inx, mx_sub), xytext=(inx, mx_sub),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05),\n",
    "            )\n",
    "    img.annotate(f'mod {modscore:.2f}', xy=(inx, modscore), xytext=(inx, modscore),\n",
    "            arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "            )\n",
    "    img.annotate(f\"{hyperparameters['scheduler'].upper()} by LR:{hyperparameters['learning_rate']}\", xy=(0, 58), xytext=(0, 58))\n",
    "    directory = f\"{hyperparameters['output_dir']}_{hyperparameters['scheduler']}\"\n",
    "    img.get_figure().savefig(f\"{directory}/maxsub-{mx_sub:.2f}_mod-{modscore:.2f}_epoch-{inx}_{hyperparameters['scheduler']}.png\")    \n",
    "    \n",
    "def training_function(hyperparameters, tokenized_datasets, tokenizer):\n",
    "    directory = f'{hyperparameters[\"output_dir\"]}_{hyperparameters[\"scheduler\"]}'\n",
    "    try:\n",
    "        repo = Repository(directory, clone_from='eleldar/train')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    if not os.path.isfile(f\"{hyperparameters['output_dir']}/{hyperparameters['file_scores']}_{hyperparameters['scheduler']}\"):\n",
    "        with open(f\"{hyperparameters['output_dir']}_{hyperparameters['scheduler']}/{hyperparameters['file_scores']}\", 'w') as file: # файл для складывания оценок\n",
    "            file.write('')\n",
    "        with open(f\"{hyperparameters['output_dir']}_{hyperparameters['scheduler']}/.gitignore\", 'w') as file:\n",
    "            file.write(\"*.png\\n\")\n",
    "    accelerator = Accelerator()\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(hyperparameters[\"model_checkpoint\"])\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, \n",
    "                                  collate_fn=data_collator, batch_size=hyperparameters['train_batch_size'])\n",
    "    eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], shuffle=False,\n",
    "                                 collate_fn=data_collator, batch_size=hyperparameters['eval_batch_size'])\n",
    "    normal_dataloader = DataLoader(tokenized_datasets[\"normal\"], shuffle=False,\n",
    "                                 collate_fn=data_collator, batch_size=hyperparameters['eval_batch_size'])\n",
    "    short_dataloader = DataLoader(tokenized_datasets[\"short\"], shuffle=False,\n",
    "                                 collate_fn=data_collator, batch_size=hyperparameters['eval_batch_size'])\n",
    "    test_dataloader = DataLoader(tokenized_datasets[\"test\"], shuffle=False,\n",
    "                                 collate_fn=data_collator, batch_size=hyperparameters['eval_batch_size'])\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=hyperparameters[\"learning_rate\"])    \n",
    "    model, optimizer, train_dataloader, eval_dataloader, normal_dataloader, short_dataloader, test_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, normal_dataloader, short_dataloader, test_dataloader\n",
    "    )\n",
    "    num_epochs = hyperparameters[\"num_epochs\"]\n",
    "    lr_schedulers = {'get_constant_schedule': get_constant_schedule(\n",
    "            optimizer=optimizer\n",
    "        ),\n",
    "            'get_constant_schedule_with_warmup': get_constant_schedule_with_warmup(\n",
    "            optimizer=optimizer, num_warmup_steps=100\n",
    "        ),\n",
    "        'get_cosine_schedule_with_warmup': get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer, num_warmup_steps=100, \n",
    "            num_training_steps=len(train_dataloader) * num_epochs,\n",
    "            num_cycles=0.5\n",
    "        ),\n",
    "        'get_cosine_with_hard_restarts_schedule_with_warmup': get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "            optimizer=optimizer, num_warmup_steps=100,\n",
    "            num_training_steps=len(train_dataloader) * num_epochs,\n",
    "            num_cycles=1\n",
    "        ),\n",
    "        'get_linear_schedule_with_warmup': get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer, num_warmup_steps=100,\n",
    "            num_training_steps=len(train_dataloader) * num_epochs,\n",
    "        ),\n",
    "        'get_polynomial_decay_schedule_with_warmup': get_polynomial_decay_schedule_with_warmup(\n",
    "            optimizer=optimizer, num_warmup_steps=100,\n",
    "            num_training_steps=len(train_dataloader) * num_epochs,\n",
    "            lr_end=1e-7, power=1.0\n",
    "        ),\n",
    "        'torch_optim_lr_scheduler_one_cycle_lr': torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optimizer, max_lr=1e-5, pct_start=1 / (num_epochs),\n",
    "            total_steps=len(train_dataloader) * num_epochs + 10, div_factor=1e+3, final_div_factor=1e+4,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "    }\n",
    "    lr_scheduler = lr_schedulers[hyperparameters['scheduler']]\n",
    "\n",
    "    # оценка перед обучением\n",
    "    evaluate(model, accelerator, eval_dataloader, note=\"sub\")\n",
    "    evaluate(model, accelerator, normal_dataloader, note=\"normal\")\n",
    "    evaluate(model, accelerator, short_dataloader, note=\"short\")\n",
    "    evaluate(model, accelerator, test_dataloader, note=\"test\")\n",
    "    \n",
    "    try:\n",
    "        repo.git_add(\".\")\n",
    "        repo.git_commit(commit_message=\"base and gitignore\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # обучение    \n",
    "    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # оценка в момент обучения\n",
    "        evaluate(model, accelerator, eval_dataloader, epoch=epoch, note=\"sub\")\n",
    "        evaluate(model, accelerator, normal_dataloader, epoch=epoch, note=\"normal\")\n",
    "        evaluate(model, accelerator, short_dataloader, epoch=epoch, note=\"short\")\n",
    "        evaluate(model, accelerator, test_dataloader, epoch=epoch, note=\"test\")        \n",
    "        \n",
    "        # Сохранение и обновление\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(directory, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(directory)\n",
    "            try:                \n",
    "                repo.git_add(\".\")                \n",
    "                repo.git_commit(commit_message=f\"Training in progress epoch {epoch}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    get_image(hyperparameters)\n",
    "    \n",
    "def decorator(function, *args):\n",
    "    '''для добавление аргументов в функцию для обучения'''\n",
    "    def wrapper():\n",
    "        return function(*args)\n",
    "    return wrapper\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")    \n",
    "training_function = decorator(training_function, hyperparameters, \n",
    "                              tokenized_datasets, tokenizer\n",
    "                             )\n",
    "notebook_launcher(training_function, num_processes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f1bf8",
   "metadata": {},
   "source": [
    "# Использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eab6bc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Companies need to buy routers to direct data traffic and connect to the internet.',\n",
       " 'ru': 'Компаниям необходимо покупать роутеры для направления трафика данных и подключения к Интернету.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['validation'][60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89d64bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Компании должны покупать маршрутизаторы для управления трафиком данных и подключения к Интернету.'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# до\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Companies need to buy routers to direct data traffic and connect to the internet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73e4cf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Компании должны покупать роутеры для управления трафиком данных и подключения к интернету.'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# после\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = hyperparameters['output_dir']\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Companies need to buy routers to direct data traffic and connect to the internet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
